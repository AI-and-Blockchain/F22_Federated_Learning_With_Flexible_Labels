{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction and reproduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract parameters from a .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "def seed_everything(seed=20):\n",
    "    \"\"\"set seed for all\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'A_model_1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/keranwang/Study/Fall 22/CSCI 4964 -- Ai & Blockchain/AI-Blockchain-Final-Project/AI Component/model_reproduction.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keranwang/Study/Fall%2022/CSCI%204964%20--%20Ai%20%26%20Blockchain/AI-Blockchain-Final-Project/AI%20Component/model_reproduction.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keranwang/Study/Fall%2022/CSCI%204964%20--%20Ai%20%26%20Blockchain/AI-Blockchain-Final-Project/AI%20Component/model_reproduction.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/keranwang/Study/Fall%2022/CSCI%204964%20--%20Ai%20%26%20Blockchain/AI-Blockchain-Final-Project/AI%20Component/model_reproduction.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sent_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mA_model_1.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keranwang/Study/Fall%2022/CSCI%204964%20--%20Ai%20%26%20Blockchain/AI-Blockchain-Final-Project/AI%20Component/model_reproduction.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sent_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keranwang/Study/Fall%2022/CSCI%204964%20--%20Ai%20%26%20Blockchain/AI-Blockchain-Final-Project/AI%20Component/model_reproduction.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m sent_model\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:579\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    577\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    580\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    581\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'A_model_1.pt'"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "import torch\n",
    "from torch import nn\n",
    "sent_model = torch.load('A_model_1.pt', map_location='cpu')\n",
    "sent_model.eval()\n",
    "for param in sent_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Linear(in_features=768, out_features=128, bias=True)\n",
      "GELU()\n",
      "Linear(in_features=128, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# get model structure\n",
    "sent_model_structure = '|'.join([layer_str.split('): ')[1] for layer_str in str(sent_model).split('\\n')[1:-1]])\n",
    "print('\\n'.join(sent_model_structure.split('|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model parameters\n",
    "with torch.no_grad():\n",
    "    sent_model_parameters = {name:val.detach() for name, val in sent_model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model uses normalization, we also need to send the mean and variance\n",
    "sent_model_parameters['0.running_mean'] = sent_model[0].running_mean\n",
    "sent_model_parameters['0.running_var'] = sent_model[0].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight  Tensor  [768]\n",
      "0.bias    Tensor  [768]\n",
      "1.weight  Tensor  [128, 768]\n",
      "1.bias    Tensor  [128]\n",
      "3.weight  Tensor  [3, 128]\n",
      "3.bias    Tensor  [3]\n",
      "0.running_mean  Tensor  [768]\n",
      "0.running_var Tensor  [768]\n"
     ]
    }
   ],
   "source": [
    "# print weight name and shape\n",
    "sent_model_parameters_names = list(sent_model_parameters.keys())\n",
    "sent_model_parameters_vals = list(sent_model_parameters.values())\n",
    "for name, val in zip(sent_model_parameters_names, sent_model_parameters_vals):\n",
    "    print(f'{name:8s}\\t{val.__class__.__name__}\\t{list(val.shape)}'.expandtabs(2))\n",
    "sent_model_parameters_names = '|'.join(sent_model_parameters_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will send 3 parameters to a client\n",
    "'''\n",
    "    sent_model_structure\n",
    "    sent_model_parameters_names\n",
    "    sent_model_parameters_vals\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Reproductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulate a client who the receives the parameters\n",
    "received_model_structure = sent_model_structure\n",
    "received_model_parameters_names = [f'[{name.split(\".\")[0]}].{name.split(\".\")[1]}'\n",
    "                                   for name in sent_model_parameters_names.split('|')]\n",
    "received_model_parameters_vals = sent_model_parameters_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "received_model = nn.Sequential(\n",
    "    *[eval('nn.' + layer) for layer in received_model_structure.split('|')]\n",
    ")\n",
    "# print part of the initial model weight\n",
    "print(received_model[0].weight.detach()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign weights to each layer\n",
    "for parameters_name, parameters_val in zip(received_model_parameters_names, received_model_parameters_vals):\n",
    "    exec(f'received_model{parameters_name} = nn.Parameter(parameters_val)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9913, 1.0620, 1.0096, 0.9389, 1.0044, 1.0548, 0.9228, 0.9387, 1.0055,\n",
      "        0.9827])\n"
     ]
    }
   ],
   "source": [
    "# make sure the weight has been changed\n",
    "print(received_model[0].weight.detach()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure two models have the same output for a given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6840, -0.6742, -0.8874],\n",
       "        [ 0.4475, -0.3338, -1.1385],\n",
       "        [ 0.6013, -0.7323, -0.7802],\n",
       "        [ 0.4355, -0.3431, -0.8839],\n",
       "        [ 0.5737, -0.6191, -0.7754]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random input for the model\n",
    "random_input = torch.randn(32, 768)\n",
    "sent_model_output = sent_model(random_input)\n",
    "sent_model_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6840, -0.6742, -0.8874],\n",
       "        [ 0.4475, -0.3338, -1.1385],\n",
       "        [ 0.6013, -0.7323, -0.7802],\n",
       "        [ 0.4355, -0.3431, -0.8839],\n",
       "        [ 0.5737, -0.6191, -0.7754]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the output of sent_model\n",
    "sent_model.eval()\n",
    "for param in sent_model.parameters():\n",
    "    param.requires_grad = False\n",
    "sent_model_output = sent_model(random_input)\n",
    "sent_model_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6840, -0.6742, -0.8874],\n",
       "        [ 0.4475, -0.3338, -1.1385],\n",
       "        [ 0.6013, -0.7323, -0.7802],\n",
       "        [ 0.4355, -0.3431, -0.8839],\n",
       "        [ 0.5737, -0.6191, -0.7754]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the output of received_model\n",
    "received_model.eval()\n",
    "for param in received_model.parameters():\n",
    "    param.requires_grad = False\n",
    "received_model_output = received_model(random_input)\n",
    "received_model_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Two models have the same output\n"
     ]
    }
   ],
   "source": [
    "if (sent_model_output == received_model_output).all():\n",
    "    print('Success! Two models have the same output')\n",
    "else:\n",
    "    print('Fail! Please check if any of the parameters are different')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab41de7c01b87720028b6e10b18ab70bba355aee4628723b15cd9ebe80125c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
