{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load project paths\n",
    "dataset_path = '../Datasets/CIFAR10'\n",
    "img_path = dataset_path + '/images'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set a fixed random seed\n",
    "from core.ai.utils import seed_everything\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data owner id of interest\n",
    "data_owner_id = 'A'\n",
    "data_owner_model_name = './saved_models/model_A_1.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.ai.utils import label_enc\n",
    "\n",
    "# Load data owner dataset\n",
    "data_owner_dataset = pd.read_excel(dataset_path + '/CIFAR10dataOwnerInfo.xlsx', sheet_name=data_owner_id)\n",
    "data_owner_dataset.image = [f'{img_path}/{image}' for image in data_owner_dataset.image]\n",
    "num_classes = data_owner_dataset.label_name.nunique()\n",
    "images, labels, label2id, id2label = label_enc(data_owner_dataset)\n",
    "\n",
    "# Create data owner's model \n",
    "from core.ai.model import get_vit_model\n",
    "vit_feature_extractor, vit_model = get_vit_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.ai.dataset import get_loader\n",
    "import albumentations as A\n",
    "eval_transform = A.Compose([\n",
    "    A.Resize(224, 224)\n",
    "])\n",
    "data_loader = get_loader(images, labels, vit_feature_extractor, eval_transform,\n",
    "                         pre_trained_model=vit_model, device=device, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Linear(in_features=768, out_features=128, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.load(data_owner_model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 422/422 [10:50<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "def predict(data_loader, model, device='cpu'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch_images, _ in tqdm(data_loader):\n",
    "        logits = model(batch_images.to(device))\n",
    "        predictions += logits.argmax(1).cpu().tolist()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(data_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample accuracy score: 0.9802\n",
      "In sample f1 score: 0.9802\n",
      "In sample weighted accuracy score: 0.9780\n",
      "In sample weighted f1 score: 0.9802\n"
     ]
    }
   ],
   "source": [
    "# Get and print evaluation metrics\n",
    "from core.ai.utils import get_metrics\n",
    "accuracy_score, f1_score, weighted_accuracy_score, weighted_f1_score = get_metrics(\n",
    "    labels, predictions, label2id)\n",
    "\n",
    "print(f'In sample accuracy score: {accuracy_score:.4f}')\n",
    "print(f'In sample f1 score: {f1_score:.4f}')\n",
    "print(f'In sample weighted accuracy score: {weighted_accuracy_score:.4f}')\n",
    "print(f'In sample weighted f1 score: {weighted_f1_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test dataset from a random seed\n",
    "SEED = 20\n",
    "np.random.seed(SEED)\n",
    "data_df = pd.read_csv(dataset_path + '/data.csv')\n",
    "test_dataset = data_df.groupby('label') .sample(frac=.2) \\\n",
    "    .query('image != \"51101.png\"') # 51101.png has a loading problem\n",
    "\n",
    "other_index = test_dataset.query('label_name not in @label2id.keys()').index\n",
    "test_dataset.loc[other_index, 'label_name'] = 'other'\n",
    "\n",
    "# Get test dataset images and labels\n",
    "images = [f'{img_path}/{image}' for image in test_dataset.image]\n",
    "labels = test_dataset.label_name.apply(lambda x: label2id[x]).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [09:44<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get test dataset data loader\n",
    "eval_data_loader = get_loader(images, labels, vit_feature_extractor, eval_transform,\n",
    "                         pre_trained_model=vit_model, device=device, shuffle=False)\n",
    "predictions = predict(eval_data_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out sample accuracy score: 0.8002\n",
      "Out sample f1 score: 0.8002\n",
      "Out sample weighted accuracy score: 0.6888\n",
      "Out sample weighted f1 score: 0.6888\n"
     ]
    }
   ],
   "source": [
    "# Get and print evaluation metrics\n",
    "from core.ai.utils import get_metrics\n",
    "accuracy_score, f1_score, weighted_accuracy_score, weighted_f1_score = get_metrics(\n",
    "    labels, predictions, label2id)\n",
    "\n",
    "print(f'Out sample accuracy score: {accuracy_score:.4f}')\n",
    "print(f'Out sample f1 score: {f1_score:.4f}')\n",
    "print(f'Out sample weighted accuracy score: {weighted_accuracy_score:.4f}')\n",
    "print(f'Out sample weighted f1 score: {weighted_f1_score:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "511e198c7044f4fa1c9fb6786af37bc0803f028a2364c2b1dc987f96133f7904"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
