{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction and reproduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract parameters from a .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "def seed_everything(seed=20):\n",
    "    \"\"\"set seed for all\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Linear(in_features=768, out_features=128, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "import torch\n",
    "from torch import nn\n",
    "sent_model = torch.load('A_model_1.pt', map_location='cpu')\n",
    "sent_model.eval()\n",
    "for param in sent_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Linear(in_features=768, out_features=128, bias=True)\n",
      "GELU()\n",
      "Linear(in_features=128, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# get model structure\n",
    "sent_model_structure = '|'.join([layer_str.split('): ')[1] for layer_str in str(sent_model).split('\\n')[1:-1]])\n",
    "print('\\n'.join(sent_model_structure.split('|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model parameters\n",
    "with torch.no_grad():\n",
    "    sent_model_parameters = {name:val.detach() for name, val in sent_model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model uses normalization, we also need to send the mean and variance\n",
    "sent_model_parameters['0.running_mean'] = sent_model[0].running_mean\n",
    "sent_model_parameters['0.running_var'] = sent_model[0].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight  Tensor  [768]\n",
      "0.bias    Tensor  [768]\n",
      "1.weight  Tensor  [128, 768]\n",
      "1.bias    Tensor  [128]\n",
      "3.weight  Tensor  [3, 128]\n",
      "3.bias    Tensor  [3]\n",
      "0.running_mean  Tensor  [768]\n",
      "0.running_var Tensor  [768]\n"
     ]
    }
   ],
   "source": [
    "# print weight name and shape\n",
    "sent_model_parameters_names = list(sent_model_parameters.keys())\n",
    "sent_model_parameters_vals = list(sent_model_parameters.values())\n",
    "for name, val in zip(sent_model_parameters_names, sent_model_parameters_vals):\n",
    "    print(f'{name:8s}\\t{val.__class__.__name__}\\t{list(val.shape)}'.expandtabs(2))\n",
    "sent_model_parameters_names = '|'.join(sent_model_parameters_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will send 3 parameters to a client\n",
    "'''\n",
    "    sent_model_structure\n",
    "    sent_model_parameters_names\n",
    "    sent_model_parameters_vals\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Reproductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulate a client who the receives the parameters\n",
    "received_model_structure = sent_model_structure\n",
    "received_model_parameters_names = [f'[{name.split(\".\")[0]}].{name.split(\".\")[1]}'\n",
    "                                   for name in sent_model_parameters_names.split('|')]\n",
    "received_model_parameters_vals = sent_model_parameters_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "received_model = nn.Sequential(\n",
    "    *[eval('nn.' + layer) for layer in received_model_structure.split('|')]\n",
    ")\n",
    "# print part of the initial model weight\n",
    "print(received_model[0].weight.detach()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign weights to each layer\n",
    "for parameters_name, parameters_val in zip(received_model_parameters_names, received_model_parameters_vals):\n",
    "    exec(f'received_model{parameters_name} = nn.Parameter(parameters_val)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9913, 1.0620, 1.0096, 0.9389, 1.0044, 1.0548, 0.9228, 0.9387, 1.0055,\n",
      "        0.9827])\n"
     ]
    }
   ],
   "source": [
    "# make sure the weight has been changed\n",
    "print(received_model[0].weight.detach()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure two models have the same output for a given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6840, -0.6742, -0.8874],\n",
       "        [ 0.4475, -0.3338, -1.1385],\n",
       "        [ 0.6013, -0.7323, -0.7802],\n",
       "        [ 0.4355, -0.3431, -0.8839],\n",
       "        [ 0.5737, -0.6191, -0.7754]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random input for the model\n",
    "random_input = torch.randn(32, 768)\n",
    "sent_model_output = sent_model(random_input)\n",
    "sent_model_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6840, -0.6742, -0.8874],\n",
       "        [ 0.4475, -0.3338, -1.1385],\n",
       "        [ 0.6013, -0.7323, -0.7802],\n",
       "        [ 0.4355, -0.3431, -0.8839],\n",
       "        [ 0.5737, -0.6191, -0.7754]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the output of sent_model\n",
    "sent_model.eval()\n",
    "for param in sent_model.parameters():\n",
    "    param.requires_grad = False\n",
    "sent_model_output = sent_model(random_input)\n",
    "sent_model_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6840, -0.6742, -0.8874],\n",
       "        [ 0.4475, -0.3338, -1.1385],\n",
       "        [ 0.6013, -0.7323, -0.7802],\n",
       "        [ 0.4355, -0.3431, -0.8839],\n",
       "        [ 0.5737, -0.6191, -0.7754]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the output of received_model\n",
    "received_model.eval()\n",
    "for param in received_model.parameters():\n",
    "    param.requires_grad = False\n",
    "received_model_output = received_model(random_input)\n",
    "received_model_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Two models have the same output\n"
     ]
    }
   ],
   "source": [
    "if (sent_model_output == received_model_output).all():\n",
    "    print('Success! Two models have the same output')\n",
    "else:\n",
    "    print('Fail! Please check if any of the parameters are different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab41de7c01b87720028b6e10b18ab70bba355aee4628723b15cd9ebe80125c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
