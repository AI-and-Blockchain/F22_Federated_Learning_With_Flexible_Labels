{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "dataset_path = '../Datasets/CIFAR10'\n",
    "img_path = dataset_path + '/images'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data owner id of interest\n",
    "data_owner_id = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=20):\n",
    "    \"\"\"set seed for all\"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import random\n",
    "    import numpy as np\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vit_model(device='cpu'):\n",
    "    # download vision transformer model \n",
    "    vit_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states=True).to(device)\n",
    "\n",
    "    # freeze the pre-trained model\n",
    "    vit_model.eval()\n",
    "    for param in vit_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return vit_feature_extractor, vit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data owner dataset\n",
    "data_owner_dataset = pd.read_excel(dataset_path + '/CIFAR10dataOwnerInfo.xlsx', sheet_name=data_owner_id)\n",
    "data_owner_dataset.image = [f'{img_path}/{image}' for image in data_owner_dataset.image]\n",
    "\n",
    "# Create data owner's model \n",
    "num_classes = data_owner_dataset.label_name.nunique()\n",
    "model = nn.Sequential(\n",
    "    nn.LazyBatchNorm1d(),\n",
    "    nn.LazyLinear(128),\n",
    "    nn.GELU(),\n",
    "    nn.LazyLinear(num_classes)\n",
    ").to(device)\n",
    "vit_feature_extractor, vit_model = get_vit_model(device)\n",
    "\n",
    "# Initialize loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding and get images and labels as lists\n",
    "def label_enc(data_owner_dataset):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder().fit(data_owner_dataset.label_name)\n",
    "    label2id = {k:v for k, v in zip(le.classes_, le.transform(le.classes_))}\n",
    "    id2label = {v:k for k, v in label2id.items()}\n",
    "    labels = le.transform(data_owner_dataset.label_name)\n",
    "    images = data_owner_dataset.image.tolist()\n",
    "    return images, labels, label2id, id2label\n",
    "\n",
    "images, labels, label2id, id2label = label_enc(data_owner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.ai.dataset import get_loader\n",
    "import albumentations as A\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.RandomCrop(224, 224),\n",
    "    A.Flip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)\n",
    "])\n",
    "data_loader = get_loader(images, labels, vit_feature_extractor, train_transform,\n",
    "                         pre_trained_model=vit_model, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Linear(in_features=768, out_features=128, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n",
      "Batch image shape: [32, 768]\n",
      "Batch label shape: [32]\n",
      "Model output shape: [32, 6]\n"
     ]
    }
   ],
   "source": [
    "# Test sample input for the model\n",
    "sample_batch = next(iter(data_loader))\n",
    "sample_images, sample_labels = sample_batch\n",
    "logits = model(sample_images.to(device))\n",
    "print(model)\n",
    "print('Batch image shape:', list(sample_images.shape))\n",
    "print('Batch label shape:', list(sample_labels.shape))\n",
    "print('Model output shape:', list(logits.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7e687a81674904825c830f6915896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2761881440454186 [tensor(6747, device='cuda:0'), 13500]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0f0beaab964aac9d45a18747131547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.1783721616482847 [tensor(7404, device='cuda:0'), 13500]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44af60f7cd1a42e590b8bb40784e544e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.1486803274866528 [tensor(7557, device='cuda:0'), 13500]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110cfbe31a814635885d821f17075573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.1271912181546904 [tensor(7712, device='cuda:0'), 13500]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b428a83d60e4459887ae2b2f450cb7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.1119315944859203 [tensor(7774, device='cuda:0'), 13500]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7378bc43023c4c9b878db67b0b4ff50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1.0891381286049342 [tensor(7804, device='cuda:0'), 13500]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffc0952d9f14dd8a14968ab5776a56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1.074612484300306 [tensor(7908, device='cuda:0'), 13500]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c5e6b5f2de42e483cbf03aff84ed7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_list = []\n",
    "    epoch_acc_sum = [0, 0]\n",
    "    for batch_images, batch_labels in tqdm(data_loader):\n",
    "        logits = model(batch_images.to(device))\n",
    "        batch_labels = batch_labels.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss_list.append(loss.item())\n",
    "        epoch_acc_sum[0] += (logits.argmax(1) == batch_labels).sum().item()\n",
    "        epoch_acc_sum[1] += len(batch_labels)\n",
    "    \n",
    "        print(f'[ {epoch:2d}:{num_epochs} ]\\tloss={np.mean(epoch_loss_list):.3f}, \\\n",
    "          acc={epoch_acc_sum[0]}/{epoch_acc_sum[1]}={epoch_acc_sum[0]/epoch_acc_sum[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d78b88c669c087ea84009c87f435080b66ea52ffb4eac29e2a7f2a91dfed40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
