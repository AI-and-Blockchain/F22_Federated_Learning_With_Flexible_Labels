{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "dataset_path = '../Datasets/CIFAR10'\n",
    "img_path = dataset_path + '/images'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data owner id of interest\n",
    "data_owner_id = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=20):\n",
    "    \"\"\"set seed for all\"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import random\n",
    "    import numpy as np\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vit_model(device='cpu'):\n",
    "    # download vision transformer model \n",
    "    vit_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states=True).to(device)\n",
    "\n",
    "    # freeze the pre-trained model\n",
    "    vit_model.eval()\n",
    "    for param in vit_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return vit_feature_extractor, vit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.1):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, (4, 4), stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyBatchNorm2d()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.LazyConv2d(64, (2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyBatchNorm2d()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.LazyConv2d(64, (1, 1), stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(256),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(out_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.conv2(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data owner dataset\n",
    "data_owner_dataset = pd.read_excel(dataset_path + '/CIFAR10dataOwnerInfo.xlsx', sheet_name=data_owner_id)\n",
    "data_owner_dataset.image = [f'{img_path}/{image}' for image in data_owner_dataset.image]\n",
    "\n",
    "# Create data owner's model \n",
    "num_classes = data_owner_dataset.label_name.nunique()\n",
    "model = CNNClassifier(in_dim=(3,32,32), out_dim=num_classes, dropout=0.1).to(device)\n",
    "\n",
    "# Initialize loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding and get images and labels as lists\n",
    "def label_enc(data_owner_dataset):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder().fit(data_owner_dataset.label_name)\n",
    "    label2id = {k:v for k, v in zip(le.classes_, le.transform(le.classes_))}\n",
    "    id2label = {v:k for k, v in label2id.items()}\n",
    "    labels = le.transform(data_owner_dataset.label_name)\n",
    "    images = data_owner_dataset.image.tolist()\n",
    "    return images, labels, label2id, id2label\n",
    "\n",
    "images, labels, label2id, id2label = label_enc(data_owner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.ai.dataset import get_loader\n",
    "import albumentations as A\n",
    "import albumentations.pytorch.transforms as T\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(32,32),\n",
    "    A.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                std=[0.2470, 0.2435, 0.2616], \n",
    "                max_pixel_value=1),\n",
    "    A.Flip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    T.ToTensorV2()\n",
    "])\n",
    "\n",
    "data_loader = get_loader(images, labels, None, train_transform, batch_size=64,\n",
    "                         pre_trained_model=None, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_out): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=12544, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Batch image shape: [64, 3, 32, 32]\n",
      "Batch label shape: [64]\n",
      "Model output shape: [64, 3]\n"
     ]
    }
   ],
   "source": [
    "# Test sample input for the model\n",
    "sample_batch = next(iter(data_loader))\n",
    "sample_images, sample_labels = sample_batch\n",
    "logits = model(sample_images.to(device))\n",
    "print(model)\n",
    "print('Batch image shape:', list(sample_images.shape))\n",
    "print('Batch label shape:', list(sample_labels.shape))\n",
    "print('Model output shape:', list(logits.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:21<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0:10 ]\tloss=0.972,           acc=2803/5900=0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:17<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1:10 ]\tloss=0.891,           acc=3222/5900=0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:21<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2:10 ]\tloss=0.856,           acc=3304/5900=0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:18<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3:10 ]\tloss=0.838,           acc=3377/5900=0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:18<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4:10 ]\tloss=0.807,           acc=3424/5900=0.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:18<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5:10 ]\tloss=0.784,           acc=3575/5900=0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:17<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6:10 ]\tloss=0.786,           acc=3514/5900=0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:17<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7:10 ]\tloss=0.767,           acc=3601/5900=0.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:17<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8:10 ]\tloss=0.756,           acc=3607/5900=0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:17<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9:10 ]\tloss=0.742,           acc=3715/5900=0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_list = []\n",
    "    epoch_acc_sum = [0, 0]\n",
    "    for batch_images, batch_labels in tqdm(data_loader):\n",
    "        logits = model(batch_images.to(device))\n",
    "        batch_labels = batch_labels.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss_list.append(loss.item())\n",
    "        epoch_acc_sum[0] += (logits.argmax(1) == batch_labels).sum().item()\n",
    "        epoch_acc_sum[1] += len(batch_labels)\n",
    "    \n",
    "    print(f'[ {epoch:2d}:{num_epochs} ]\\tloss={np.mean(epoch_loss_list):.3f}, \\\n",
    "          acc={epoch_acc_sum[0]}/{epoch_acc_sum[1]}={epoch_acc_sum[0]/epoch_acc_sum[1]:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../saved_models/model_B_2.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "511e198c7044f4fa1c9fb6786af37bc0803f028a2364c2b1dc987f96133f7904"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
